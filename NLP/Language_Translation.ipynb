{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqcfO/ZbOQlVMznu56cUPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santoshrsarangi/tensorflow/blob/main/NLP/Language_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "poMjT7Iiyf4N",
        "outputId": "7981180a-090a-4309-e937-3d9c22756326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-10 23:26:24--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.128, 74.125.137.128, 142.250.141.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-06-10 23:26:24 (270 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "1q36y4WezGcG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\""
      ],
      "metadata": {
        "id": "ohvQ6Y4zzMeZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1] "
      ],
      "metadata": {
        "id": "nIZ5Qvoe0Apk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []"
      ],
      "metadata": {
        "id": "J1KWEdR00Ot6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"spa-eng/san.txt\", 'w') as w:\n",
        "#  w.writelines(\"Santosh sarangi \\n123 780 \\nHello \\nZebra animal\")"
      ],
      "metadata": {
        "id": "jHGeCFhC0ZLp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(\"spa-eng/san.txt\") as f:\n",
        "#  lines = f.read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "6NkLOqeV4DxG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lines"
      ],
      "metadata": {
        "id": "lfxaeprS4Z2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in lines:\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "3vLStvSa7TnM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "gkV-7btOoYYm",
        "outputId": "0cbda224-1a28-400c-ca12-d06521c62fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('He told me his address, but unfortunately I had no paper to write it down on.', '[start]Él me dijo su dirección, pero desafortunadamente yo no tenía un papel en que anotarlo.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "s70Ngjwmozmf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))"
      ],
      "metadata": {
        "id": "R4Mkxq_7o62S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = len(text_pairs) - 2 * num_val_samples"
      ],
      "metadata": {
        "id": "kj7z20_3pJlw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = text_pairs[:num_train_samples]"
      ],
      "metadata": {
        "id": "YRJJ9RvYpX75"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pairs = text_pairs[num_train_samples: num_train_samples+num_val_samples]"
      ],
      "metadata": {
        "id": "IwSWClGApgD8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
      ],
      "metadata": {
        "id": "qWd4t0RZppGX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "pscnDiQXpvk5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "H3BssgvIp1hR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Myjd_QwBp3Su"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\""
      ],
      "metadata": {
        "id": "w9Y7b9Vnp4kT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n"
      ],
      "metadata": {
        "id": "7A04MkCxqHXF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
        "  )"
      ],
      "metadata": {
        "id": "hWKZkDgpqaEt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 25000\n",
        "sequence_length = 20"
      ],
      "metadata": {
        "id": "hGG3ECsbq1fq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "lIxYQFr8q84h"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    standardize=custom_standardization\n",
        ")"
      ],
      "metadata": {
        "id": "B-EMohlZriry"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_english_text = [pair[0] for pair in train_pairs]"
      ],
      "metadata": {
        "id": "tfANvb8Ir8cA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spanish_text = [pair[1] for pair in train_pairs]"
      ],
      "metadata": {
        "id": "mplQka08s_Gc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization.adapt(train_english_text)"
      ],
      "metadata": {
        "id": "HbMMHhMdtFYT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_vectorization.adapt(train_spanish_text)"
      ],
      "metadata": {
        "id": "mv3EwIB8tLND"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twaKgzSLtP5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}